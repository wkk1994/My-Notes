# 注意力稀释（Attention Dilution）

**当把太多段落（chunk）一次性塞进 prompt，模型反而像“同时听 20 个人说话”一样，谁也听不清，结果回答质量下降。**

## 一、为什么会出现注意力稀释？

1. **注意力机制本质**
   - Transformer 的注意力是 **对输入序列的每个 token 分配一个权重分布**。
   - 当输入序列很长（比如几千个 token），模型需要在所有 token 上分配注意力，权重会被稀释。

2. **无关或冗余信息**
   - 在 RAG 场景下，如果检索到的文档里混入很多不相关内容，模型会“浪费注意力”在无关片段上，降低对关键信息的聚焦。

3. **Chunking 粒度问题**
   - 如果 chunk 太大，里面包含的信息过多，相关信号会被淹没在大量 token 中。  
   - 如果 chunk 太小，语义可能割裂，导致注意力无法有效聚合。  

---

## 二、注意力稀释的影响
- **答案不准确**：模型可能引用了不相关的内容，甚至“脑补”结果。  
- **幻觉增加**：模型在稀释环境下可能混合无关片段，生成看似合理但错误的答案。  
- **性能下降**：长上下文带来的计算开销更大，但有效信息反而没突出。  

---

## 三、解决注意力稀释的思路
1. **检索优化**
   - 用 **Rerank** 模型（如 BERT-based cross-encoder）过滤掉低相关 chunk。  
   - 控制返回的文档数量（Top-k 不是越多越好，一般 3~5 比较合理）。  

2. **上下文压缩（Context Compression）**
   - 在把文档交给 LLM 之前，先做一次摘要或提取要点，减少无关 token。  

3. **Chunking 策略优化**
   - 合理设置 chunk_size 和 overlap（比如 512 token + 50 overlap）。  
   - 采用语义分段而不是纯文本切割，保持上下文连贯。  

4. **路由/分组查询**
   - 对于复杂问题，把 query 拆解成子问题，分别检索和回答，再合成结果。  

5. **模型侧优化**
   - 使用 **长上下文专门优化过的模型**（如 Claude 3.5 Sonnet, GPT-4-turbo, Llama 3.1 405B with long context）。  
   - 但长上下文 ≠ 有效利用，检索和压缩仍然关键。  

---

## 实践中的坑

1. 稀释不一定发生在“最长”那次调用

同一轮对话里如果先给 5 个 chunk 答对了，用户追问一次又把上一轮 5 个 chunk 再原封不动带进去，模型会把“自己刚生成的答案”也当成噪声，从而第二次回答反而跑偏。
→ 工程做法：历史上下文只保留“最终答案”，把用过的 chunk 全部截断。

2. Rerank 模型本身也会过拟合“表面相似”

我们曾在医疗场景里发现，cross-encoder 把带“糖尿病”关键词的说明书排在真正描述“糖尿病足分级”的指南前面，导致 LLM 被带歪。
→ 缓解：在 rerank 训练集里加 10 % 负样本，保证“关键词命中但语义无关”的被压分。

3. 摘要≠无损压缩

用第二个 LLM 先对 chunk 做摘要，虽然 token 少了，但摘要模型会引入 2 %～4 % 的幻觉，二次放大误差。
→ 折中方案：
只给 chunk 的“标题+首句+带高亮命中词”三件套，不重新生成句子；
或者让摘要模型输出 bullet，并强制附原始句编号，方便最终 LLM 回源校验。

📌 **一句话总结**  
注意力稀释 = 模型在长上下文中注意力分散，导致关键信息被淹没。  
解决办法是 **减少无关信息（检索/压缩/过滤）**，并 **优化 chunking 和上下文组织方式**。  
