[toc]

# 第13课 性能与SQL优化2

## 1.MySQL 事务与锁*

### 事务的可靠模型ACID

* Atomicity: 原子性，一次事务中要么全部成功，要么全部失败。
* Consistency: 一致性，跨表、跨行、跨事务，数据库始终保持一致性状态。
* Isolation: 隔离性，可见性，保护事务不会相互干扰，包含4种隔离级别。
* Durability: 持久性，事务一旦提交成功后，不会丢失数据。

### 表级锁

意向锁：表明事务稍后要进行哪种类型的锁定

* 共享意向锁（IS）：打算在某些行上设置共享锁。
* 共享排它锁（IX）：打算对某些行设置排他锁。
* Insert意向锁：Insert操作设置的间隙锁。

**意向锁协议：**

* 事务在获取表中某行的共享锁之前，必须要获得该表的IS锁，或者限制性更强的锁。
* 事务在获取表中某行的排他锁之前，必须要获得该表的IX锁。

**意向锁与其他锁的兼容性如下：**

||X|IX|S|IS|
|:--:|:--:|:--:|:--:|:--:|
|X|冲突|冲突|冲突|冲突|
|IX|冲突|兼容|冲突|兼容|
|S|冲突|冲突|兼容|兼容|
|IS|冲突|兼容|兼容|兼容|

### 行级锁

* 记录锁（Record）：始终锁定索引记录，注意隐藏的聚簇索引;
* 间隙锁（Gap Lock）：是对索引记录之间进行锁定，或者是对第一个索引记录之前的间隙进行锁定，或者是对最后一个索引之后的间隙锁定。
* 临键锁（Next-Key Lock）：是索引记录锁加上前面的间隙锁组合而成的。
* 谓词锁(Predicat): 空间索引。

**间隙锁是性能和并发之间的一种权衡, 只会在某些事务隔离级别中使用。**

### 事务的隔离级别

* 读未提交（Read Uncommit）
  很少使用，会有脏读、幻读、不可重复度读的问题。

* 读已提交（Read Commit）
  * 每次查询都会设置和读取自己的新快照。
  * 仅支持基于行的bin-log
  * UPDATE 优化: 半一致读(semi-consistent read)
  * 会有不可重复读，幻读的问题。

* 可重复读（Repeatable Read）
  * Innodb的默认事务隔离级别
  * 在事务开始的第一个查询语句时创建快照
  * 使用MVCC（多版本并发控制）
  * 可能的问题: InnoDB 不能保证没有幻读, 需要加锁

* 串行化（Serializable）
  最严格的级别，事务串行执行，资源消耗最大；这样就不会产生脏读、幻读、不可重复读的问题。

**事务的隔离级别是数据库的基本特征**

MySQL的事务隔离级别：

* Innodb的默认事务隔离级别是可重复读
* 可以设置全局的默认事务隔离级别，`set global transaction_isolation = 'REPEATABLE-READ';`
* 可以单独设置会话的隔离级别，`set transaction_isolation = 'REPEATABLE-READ';`

### undo Log（撤销日志）

undo log的作用：

* 保证事务的原子性
  通过事务回滚，来保证事务的原子性。
* 记录事务回滚时所需的撤消操作
  * 一条 INSERT 语句，对应一条 DELETE 的 undo log
  * 每个 UPDATE 语句，对应一条相反 UPDATE 的 undo log
* 多行版本控制(MVCC)

保存位置：

* system tablespace (MySQL 5.7默认)
* undo tablespaces (MySQL 8.0默认)

### redo log（重做日志）

* 保证事务的持久性，防止事务提交后数据未刷新到磁盘就掉电或者崩溃。
* 物理日志，记录事务对数据做了什么修改。
* 通过WAL（write-Ahead-Loinging）技术，提升性能，先写日志，再写磁盘。
* 日志文件：ib_logfile0, ib_logfile1
* 日志缓冲: innodb_log_buffer_size
* 强刷: fsync()

### MVCC: 多版本并发控制

* 使Innodb支持一致性读：Read Committed和Repeatable Read。
* 让查询不被阻塞，不需要等待其他事务持有的锁，这种技术可以增加并发性能。
* Innodb保留被修改行的旧版本。
* 查询正在被其他事务更新的数据时，会读取更新之前的版本。
* 每行数据都存在一个版本号，每次更新时都更新该版本。
* 这种技术在数据库领域的使用并不普遍。 某些数据库, 以及某些 MySQL 存储引擎都不支持。

聚簇索引的更新 = 替换更新
二级索引的更新 = 删除+新建

MVCC的实现机制：

* 隐藏列
  ![MVCC隐藏列](https://note.youdao.com/yws/api/personal/file/WEB495cb832a32e6e698c667881d2aceec3?method=download&shareKey=b7241ee6faf66cc91c3dd79b3985dea9)

* 事务链表，保存还未提交的事务，事务提交则会从链表中摘除。
* Read view: 每个 SQL 一个, 包括 rw_trx_ids, low_limit_id, up_limit_id, low_limit_no 等
* 回滚段: 通过 undo log 动态构建旧版本数据

## 2.DB 与 SQL 优化*

### 如何设计表

* 数据类型的选择
  
  数据类型并不是越大越好，越大的数据类型占用的空间越大，较小的数据类型可以节省更多的磁盘空间，节省更多的钱。

* 存储引擎的选择

  选择合适的存储引擎：
  InnoDB：聚集索引；锁粒度是行锁；Innodb支持事务。
  TokuDB：高压缩比，尤其适用于压缩和归档（1:12）；在线添加索引，不影响读写操作；支持完整的ACID特性和事务机制。

  **没有其他特别因素就使用InnoDB**

### 如何发现需要优化的 SQL

* 慢查询日志
  `slow_query_log`: 控制慢查询日志是否开启
  `slow_query_log_file`: 指定慢查询日志的文件地址
  `long_query_time`: 慢查询的时间
  `log_queries_not_using_indexes`: 控制不使用索引的查询是否记录到慢查询日志中

* 监控

### 修改表结构的危害

* 索引重建
* 锁表
* 抢占资源
* 主从延迟

### 写入优化

大批量写入优化

* PreparedStatement减少SQL解析
* Multiple Values/Add Batch 减少交互
  insert into t(),()...这种对SQL解析不友好，每个()都需要进行SQL解析。
* Load Data，直接导入
* 对于批量导入，可以先取消索引和约束检查。

> select * from  lockt where (id,col)=(1,1);

### 数据更新

谨慎使用数据范围更新，范围太大会导致GAP Lock太大，影响其他更新或插入的性能。

### 模糊查询

like查询的前缀有%的话不能走索引，可以使用solr/ES进行全文检索。

### 连接查询

使用小表作为驱动表，避免笛卡尔积。

### 索引失效

导致索引失效的情况：

* NULL，not，not in，函数等
* 减少使用 or，可以用 union（注意 union all 的区别），以及like的使用
* 大数据量下，所有条件的组合查询想要都走索引，使用es等。
* 必要的时候使用force index来强制查询走某个索引。

### 查询 SQL 到底怎么设计

查询数据量和查询次数的平衡
避免不必须的大量重复数据传输
避免使用临时文件排序或临时表
分析类需求，可以用汇总表

## 3.常见场景分析*

### 怎么实现主键ID

* 自增主键：实现简单，不支持分布式。
* sequence：和自增一样的问题。
* 模拟sequene：业务可控，实现复杂一些，可能会出现主键空洞。
* UUID：实现简单，永不重复，但是不是单调递增，占用空间大。
* 时间戳/随机数：可能会重复。
* snowflake：主机id+时间戳+递增数

> 自增主键，sequence可能出现数据量泄漏风险，通过一天中前后主键的相减就能获得今天一天的订单数，再根据以往的订单的平均金额，就可以获得一天的交易额!!!很危险。

### 高效分页

* 常见的分页插件的实现是使用SQL，嵌套一个count，这种查询方式有性能问题。
  `select count(*) from (SQL);`这种查询方式SQL中有很多不需要的字段完全没必要参与到行的计算，以及order by也不需要参与计算。
  改进方式：
  * 重写count方式。
* 大数量级分页的问题，limit 1000000,20;
  对于大数据量级别的分页，limit 1000000,20需要跳过1000000条数据，这种很慢。
  改进方式：
  * 反序，如果原来的数据是升序查询的，limit 1000000已经接近数据的末尾了，可以通过倒序查询，只需要跳过很少的数据就可以获取到目标数据。
  * 非精确分页，每次查找下一页或者后面几页时，带上当前页的最后一个id，这样查找下一页的查询语句就可以写成：`select * from t where id > last_id limit 10`，通过这种查询方式可以很快定位到接下来的数据，不需要在跳过很多数据。
  这种查询的问题是，如果在查询的过程中，当前最后一个id的的前面插入了数据，那么新的数据就不能被后面的分页查询到，所以是分精确分页。

### 乐观锁与悲观锁

悲观锁：
select * from xxx for update;
update xxx;
commit;

悲观锁虽然避免了数据竞争导致的数据混乱问题，但是锁的粒度比较大，对性能有影响。早期的mq等消息中间件都是通过这种方式实现资源的竞争管理。

乐观锁：
select * from xxx;
update xxx where value=oldValue;

乐观锁适合在资源或数据竞争不大的情况下使用，注意ABA问题，可以通过加上version来避免ABA问题。

## Tips

> crash-safe
> Read view
> TokuDB

## 问题

* undo log是怎么保证事务的原子性的，事务的原子性是一个事务中操作，要么全部成功，要么全部失败，如果有一个操作失败，那么就需要通过undo log来进行事务的回滚，所以undo log只是提供了事务回滚的作用，通过事务回滚间接实现事务的原子性。对于事务的提交成功的原子性，undo log不提供这个功能。
* 二级索引的更新，怎么操作的，mvcc怎么保证数据查询走二级索引问题。
