# 特征工程

特征工程是数据工程的一部分，但是它是灵魂，而且也和模型工程紧密相关。实际上特征工程和模型工程没有明显的边界，许多特征工程的动作在足够复杂的模型中已经被自动化处理了，要明白的一点是特征工程的目的是为了帮助模型减轻压力。

**特征工程的核心工作**特征处理的过程是对数据进行微观和宏观投影的过程，所以虽然叫特征处理，但特征本身其实没有变化，变的只是你观察的维度。

## 特征的特征

通常情况下，合理的数据转换能够帮助我们更好的理解样本数据。比如一批年龄特征，看上去没有规律不好理解，但是我们可以转换成老年、中年、青年这样的分类，帮助模型快速学习到宏观的特点。

同样的道理，如果我们将数值型特征直接给到线性模型，可能会导致模型很难发现特征中的一些非线性特点，这时你可以让它做幂函数、指数函数、三角函数等等这些变化之后再给到线性模型，就可以更好地处理非线性特征了。
## 独热编码

如果我们要将性别特征输入到模型中，通常做法直接使用枚举值0、1、2表示男生、女生和未知，但是这样传给模型，会让模型认为这个特征有可比性（有大小关系），会导致模型可能学完所有的训练数据后，没搞懂这个特征的关系，从而可能放弃这个重要的特征。这个时候直接使用枚举或数字表示特征就不合适，而独热编码恰好是解决这一问题的方法。

独热编码是将每个类别都会被转换为一个二进制向量，这个向量的长度等于类别的总数，并且向量中只有一个位置是1，其他位置都是0。这个1所在的位置对应于该类别在类别列表中的索引。比如男生、女生和未知使用独热编码可以表示为[1,0,0],[0,1,0],[0,0,1]，这样每个向量都是正交，保证了空间上的相互独立。

## 空间关系

特征进行编码后，怎么表示特征之间的关系，就是把现实世界中的关系映射到特征模型中。比如性别和兴趣，男生更喜欢钓鱼，钓鱼一般又何露营关系比较近。

**预训练模型：是用来将真实世界中的各个实体之间的关系，让模型提前知晓。**

具体预训练模型怎么做的？就是通过限制网络上的大量的语料来学习，计算出相邻单词之间的字面距离，以此来表示它们之间的空间关系。

衡量词与词之间的关系通常有两种方法：第一种是跳字模型 Skip-gram，第二种是连续词袋模型 CBOW（Continuous Bag of Words）。

简单来说，跳字模型的目标是，通过给定一个中心词用它来预测前后单词；而在连续词袋模型中，模型的目标是通过前后词来预测中心词。这里有一个假设，那就是在字面上相近位置经常出现的单词之间，相关性会更大。

随着语料不断增加，每个单词后面的链条也会越来越长。这样，模型就能通过统计，知道哪些单词之间的关系更相近了。

这就是 Word2Vec 的思想，也是 NLP 预训练模型（PTM）的重要工作。现在的 GPT 系列最早也是从 Word2Vec 一步步发展出来的，所以它的重要性不言而喻。



> https://zhuanlan.zhihu.com/p/685077556
> https://zhuanlan.zhihu.com/p/39012149